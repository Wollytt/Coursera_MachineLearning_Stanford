{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning: Programming Exercise 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scipy.io\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks Learning\n",
    "In this exercise, you will implement the backpropagation algorithm for neural networks and apply it to the task of hand-written digit recognition.\n",
    "\n",
    "## 1. Neural Networks\n",
    "\n",
    "In the previous exercise, you implemented feedforward propagation for neural networks and used it to predict handwritten digits with the weights we provided. In this exercise, you will implement the backpropagation algorithm to learn the parameters for the neural network.\n",
    "\n",
    "### 1.1 Visualizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 400)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat = scipy.io.loadmat('ex4data1.mat')\n",
    "X = mat['X']\n",
    "y = mat['y']\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0xb1ba734a8>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0xb1bafe5f8>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(-0.5, 19.5, 19.5, -0.5)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0xb1bafe9e8>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0xb211c3080>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(-0.5, 19.5, 19.5, -0.5)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0xb211b1ef0>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0xb1fd657b8>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(-0.5, 19.5, 19.5, -0.5)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0xb1fd65b70>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0xb1fd65ac8>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(-0.5, 19.5, 19.5, -0.5)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0xb1fd8ae80>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0xb1eb1b748>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(-0.5, 19.5, 19.5, -0.5)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0xb1eb1b5f8>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0xb1eb6cf28>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(-0.5, 19.5, 19.5, -0.5)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0xb1eb6ce48>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0xb19492828>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(-0.5, 19.5, 19.5, -0.5)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0xb19492cf8>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0xb194c3f60>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(-0.5, 19.5, 19.5, -0.5)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0xb194cf3c8>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0xb179c0dd8>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(-0.5, 19.5, 19.5, -0.5)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x103172080>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0xb1ba21b70>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(-0.5, 19.5, 19.5, -0.5)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0xb1b9c6a20>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0xb1b9d2208>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(-0.5, 19.5, 19.5, -0.5)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0xb1ba217f0>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0xb1b6dacc0>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(-0.5, 19.5, 19.5, -0.5)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 12 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "example_nums = 12\n",
    "idxs = np.random.choice(range(X.shape[0]), example_nums, replace=False)\n",
    "size = round(math.sqrt(X.shape[1]))\n",
    "\n",
    "for i, idx in enumerate(idxs):\n",
    "    plt.subplot(3, 4, i+1)\n",
    "    plt.imshow(X[idx].reshape(size, size).T.astype('float'))\n",
    "    plt.axis('off')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Model representation\n",
    "\n",
    "Our neural network is shown in Figure 2. It has 3 layers- an input layer, a hidden layer and an output layer. Recall that our inputs are pixel values of digit images. Since the images are of size 20 x 20, this gives us 400 input layer units (not counting the extra bias unit which always outputs +1). \n",
    "\n",
    "You have been provided with a set of network parameters $(\\Theta^{(1)},\\Theta^{(2)})$ already trained by us. These are stored in ex4weights.mat.  Run the code below to load them into Theta1 and Theta2. The parameters have dimensions that are sized for a neural network with 25 units in the second layer and 10 output units (corresponding to the 10 digit classes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25, 401)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(10, 26)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = scipy.io.loadmat('ex4weights.mat')\n",
    "Theta1 = weights['Theta1']\n",
    "Theta2 = weights['Theta2']\n",
    "Theta1.shape\n",
    "Theta2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Feedforward and cost function\n",
    "Now you will implement the cost function and gradient for the neural network. First, complete the code in nnCostFunction.m to return the cost. Recall that the cost function for the neural network (without regularization) is\n",
    "\n",
    "<center> $J(\\theta) =\\frac{1}{m}{\\sum_{i=1}^m\\sum_{k=1}^K{\\left[-y_k^{(i)} \\log((h_{\\theta}(x^{(i)}))_k)- (1 -y_k^{(i)}) \\log(1- (h_{\\theta}(x^{(i)}))_k)\\right]}}$ </center>\n",
    "\n",
    "where $h_\\theta(x^{(i)})$ is computed as shown in the Figure 2 and $K = 10$ is the total number of possible labels. Note that $h_\\theta(x^{(i)})_k = a^{(3)}_k$ is the activation (output value) of the -th output unit. Also, recall that whereas the original labels (in the variable y) were  for the purpose of training a neural network, we need to recode the labels as vectors containing only values 0 or 1, so that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1./(1.+np.exp(-1.*z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nnCostFunction(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, reg):\n",
    "    '''\n",
    "    NNCOSTFUNCTION Implements the neural network cost function for a two layer\n",
    "    neural network which performs classification\n",
    "    [J grad] = NNCOSTFUNCTON(nn_params, hidden_layer_size, num_labels, X, y, lambda) \n",
    "    computes the cost and gradient of the neural network. \n",
    "    The parameters for the neural network are \"unrolled\" into the vector nn_params \n",
    "    and need to be converted back into the weight matrices. \n",
    "    The returned parameter grad should be a \"unrolled\" vector of the partial derivatives of the neural network.\n",
    "    '''\n",
    "    # Reshape nn_params back into the parameters Theta1 and Theta2, \n",
    "    # the weight matrices for our 2 layer neural network\n",
    "    Theta1 = nn_params[:(input_layer_size+1)*hidden_layer_size].reshape(hidden_layer_size, input_layer_size+1)\n",
    "    Theta2 = nn_params[(input_layer_size+1)*hidden_layer_size:].reshape(num_labels, hidden_layer_size+1)\n",
    "    \n",
    "    # Set up some useful variables\n",
    "    m = X.shape[0]\n",
    "    \n",
    "    \n",
    "    # Part 1: Feedforward the neural network and return the cost in the variable J\n",
    "    X = np.concatenate((np.ones((m,1)), X), axis=1) # 5000*401\n",
    "    z2 = np.dot(Theta1, X.T) # (25*401)*(401*5000)\n",
    "    a2 = sigmoid(z2) # (25*5000)\n",
    "\n",
    "    a2 = np.concatenate((np.ones((m,1)), a2.T), axis=1) #(5000*26)\n",
    "    z3 = np.dot(Theta2, a2.T) #(10*26)*(26*5000)\n",
    "    h_theta = sigmoid(z3) # h_theta equals a3, (10*5000)\n",
    "    \n",
    "    # Recode the labels as vectors containing only values 0 or 1 \n",
    "    y_new = np.zeros((num_labels, m)) # (10*5000)\n",
    "    for i in range(m):\n",
    "        y_new[y[i]-1, i] = 1\n",
    "    \n",
    "    \n",
    "    # Cost without regularization\n",
    "    J = -((y_new*np.log(h_theta)).sum()+((1-y_new)*np.log(1-h_theta)).sum())/m\n",
    "    # Add regularization\n",
    "    J += reg*np.sum(Theta1[:,1:]**2)/(2*m) + reg*np.sum(Theta2[:,1:]**2)/(2*m) \n",
    "    \n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at parameters (loaded from ex4weights):  0.2876291651613189\n"
     ]
    }
   ],
   "source": [
    "input_layer_size  = 400   # 20x20 Input Images of Digits\n",
    "hidden_layer_size = 25    # 25 hidden units\n",
    "num_labels = 10           # 10 labels, from 1 to 10 (note that we have mapped \"0\" to label 10)\n",
    "\n",
    "# Unroll parameters \n",
    "nn_params = np.concatenate((Theta1.flatten(), Theta2.flatten()))\n",
    "\n",
    "# Weight regularization parameter (we set this to 0 here).\n",
    "reg = 0 \n",
    "\n",
    "J = nnCostFunction(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, reg) \n",
    "\n",
    "print('Cost at parameters (loaded from ex4weights): ', J) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Regularized cost function\n",
    "\n",
    "The cost function for neural networks with regularization is given by \n",
    "<center>$J(\\theta) =\n",
    "\\frac{1}{m}\\sum_{i=1}^m\n",
    "\\sum_{k=1}^K \n",
    "\\left[ -y^{(i)}_k \\log((h_{\\theta}(x^{(i)})_k)- (1 -y^{(i)}_k) \\log(1-(h_{\\theta}(x^{(i)}))_k)  \\right]\n",
    "\\\\ \\qquad \n",
    "+\\frac{\\lambda}{2m} \\left[\\sum_{j=1}^{25} \n",
    "\\sum_{k=1}^{400} {\\left( \\Theta_{j,k}^{(1)}\\right)^2}+\n",
    "\\sum_{j=1}^{10} \n",
    "\\sum_{k=1}^{25} {\\left( \\Theta_{j,k}^{(2)}\\right)^2}\\right]$</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at parameters (loaded from ex4weights):  0.38376985909092365\n"
     ]
    }
   ],
   "source": [
    "# Weight regularization parameter (we set this to 1 here).\n",
    "reg = 1;\n",
    "\n",
    "J = nnCostFunction(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, reg)\n",
    "print('Cost at parameters (loaded from ex4weights): ', J) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Sigmoid gradient\n",
    "To help you get started with this part of the exercise, you will first implement the sigmoid gradient function. The gradient for the sigmoid function can be computed as \n",
    "<center>$g'(z)=\\frac{d}{dz}g(z) = g(z)(1-g(z))$</center>\n",
    "where \n",
    "<center>$\\mathrm{sigmoid}(z)=g(z)=\\frac{1}{1+e^{-z}}$</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoidGradient(z):\n",
    "    return sigmoid(z)*(1-sigmoid(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Call your sigmoidGradient function\n",
    "sigmoidGradient(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Random initialization\n",
    "When training neural networks, it is important to randomly initialize the parameters for symmetry breaking. One effective strategy for random initialization is to randomly select values for $\\Theta^{(l)}$ uniformly in the range $[-\\epsilon_{int},\\epsilon_{init}]$. You should use $\\epsilon_{init} = 0.12$*. This range of values ensures that the parameters are kept small and makes the learning more efficient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randInitializeWeights(L_in, L_out):\n",
    "    epsilon_init = 0.12\n",
    "    W = np.random.rand(L_out, 1 + L_in)*2*epsilon_init - epsilon_init\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_Theta1 = randInitializeWeights(input_layer_size, hidden_layer_size);\n",
    "initial_Theta2 = randInitializeWeights(hidden_layer_size, num_labels);\n",
    "\n",
    "#  Unroll parameters\n",
    "initial_nn_params = np.concatenate((initial_Theta1.flatten(), initial_Theta2.flatten()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*One effective strategy for choosing $\\epsilon_{init}$ is to base it on the number of units in the network. A good choice of $\\epsilon_{init}$ is $\\epsilon_{init} = \\frac{\\sqrt{6}}{\\sqrt{L_{in}+L_{out}}}$ where $L_{in} = s_l$ and $L_{out} = s_l+1$ are the number of units in the layers adjacent to $\\Theta^{(l)}$.\n",
    "\n",
    "### 2.3 Backpropagation\n",
    "Now, you will implement the backpropagation algorithm. Recall that the intuition behind the backpropagation algorithm is as follows. Given a training example $(x^{(t)}, y^{(t)})$, we will first run a 'forward pass' to compute all the  activations throughout the network, including the output value of the hypothesis $h_\\Theta(x)$. Then, for each node $j$ in layer $l$, we would like to compute an 'error term' $\\delta^{(l)}_j$ that measures how much that node was 'responsible' for any errors in our output. \n",
    "\n",
    "For an output node, we can directly measure the difference between the network's activation and the true target value, and use that to define $\\delta^{(3)}_j$ (since layer 3 is the output layer). For the hidden units, you will compute $\\delta^{(l)}_j$ based on a weighted average of the error terms of the nodes in layer $l+1$. \n",
    "\n",
    "In detail, here is the backpropagation algorithm (also depicted in Figure 3). You should implement steps 1 to 4 in a loop that processes one example at a time. Concretely, you should implement a for loop for t = 1:m and place steps 1-4 below inside the for loop, with the $t^{th}$ iteration performing the calculation on the $t^{th}$ training example $(x^{(t)}, y^{(t)})$. Step 5 will divide the accumulated gradients by $m$ to obtain the gradients for the neural network cost function.\n",
    "\n",
    "1. Set the input layer's values $(a^{(1)})$ to the -th training example $x^{(t)}$. Perform a feedforward pass (Figure 2), computing the activations $(z^{(2)}, a^{(2)}, z^{(3)}, a^{(3)})$ for layers 2 and 3. Note that you need to add a  term to ensure that the vectors of activations for layers $a^{(1)}$ and $a^{(2)}$ also include the bias unit. \n",
    "\n",
    "2. For each output unit $k$ in layer 3 (the output layer), set $\\delta^{(3)}_k = (a_k^{(3)}-y_k)$ where $y_k\\in \\{0, 1\\}$ indicates whether the current training example belongs to class $k$ $(y_k = 1)$, or if it belongs to a different class $(y_k = 0)$.You may find logical arrays helpful for this task (explained in the previous programming exercise).\n",
    "\n",
    "3. For the hidden layer $l=2$, set $\\delta^{(2)} = (\\Theta^{(2)})^T \\delta^{(3)}.*g'(z^{(2)})$\n",
    "\n",
    "4. Accumulate the gradient from this example using the following formula: $\\Delta^{(l)} = \\Delta^{(l)} + \\delta^{(l+1)} (a^{(l)})^T$. Note that you should skip or remove $\\delta_0^{(2)}$\n",
    "\n",
    "5. Obtain the (unregularized) gradient for the neural network cost function by dividing the accumulated gradients by $\\frac{1}{m}:$ $\\frac{\\partial}{\\partial\\Theta^{(l)}_{ij}} J(\\Theta)= D^{(l)}_{ij}= \\frac{1}{m}\\Delta^{(l)}_{ij}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nnGradient(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, reg):\n",
    "    Theta1 = nn_params[:(input_layer_size+1)*hidden_layer_size].reshape(hidden_layer_size, input_layer_size+1)\n",
    "    Theta2 = nn_params[(input_layer_size+1)*hidden_layer_size:].reshape(num_labels, hidden_layer_size+1)\n",
    "    \n",
    "    m = X.shape[0]\n",
    "    \n",
    "    y_new = np.zeros((num_labels, m)) # (10*5000)\n",
    "    for i in range(m):\n",
    "        y_new[y[i]-1, i] = 1\n",
    "    \n",
    "    Theta1_grad = np.zeros(Theta1.shape)\n",
    "    Theta2_grad = np.zeros(Theta2.shape)\n",
    "    \n",
    "    X = np.concatenate((np.ones((m,1)), X), axis=1) \n",
    "    for t in range(m):\n",
    "        # Step 1\n",
    "        a1 = X[t, :] # (1*401)\n",
    "        z2 = np.dot(Theta1, np.array([a1]).T) # (25*401)*(401*1)\n",
    "        a2 = sigmoid(z2) # (25*1)\n",
    "        \n",
    "        a2 = np.concatenate((np.ones((1,1)), a2), axis=0) # adding a bias (26*1)\n",
    "        z3 = np.dot(Theta2, a2) # (10*26)*(26*1)\n",
    "        a3 = sigmoid(z3) # (10*1)\n",
    "        \n",
    "        # Step 2\n",
    "        delta_3 = a3 - y_new[:,t].reshape((10,1)) # (10*1)\n",
    "        \n",
    "        # Step 3\n",
    "        z2 = np.concatenate((np.ones((1,1)), z2), axis=0)\n",
    "        delta_2 = np.dot(Theta2.T, delta_3) * sigmoidGradient(z2) # ((26*10)*(10*1))=(26*1)\n",
    "        \n",
    "        # Step 4\n",
    "        delta_2 = delta_2[1:] # skipping sigma2(0) (25*1)\n",
    "\n",
    "        Theta2_grad = Theta2_grad + np.dot(delta_3, a2.T) # (10*1)*(1*26)\n",
    "        Theta1_grad = Theta1_grad + np.dot(delta_2, a1.reshape((1,401))) # (25*1)*(1*401)\n",
    "        \n",
    "    # Step 5\n",
    "    Theta2_grad = (1/m) * Theta2_grad # (10*26)\n",
    "    Theta1_grad = (1/m) * Theta1_grad # (25*401)\n",
    "    \n",
    "    Theta1_grad[:,1:] = Theta1_grad[:, 1:] + ((reg/m) * Theta1[:,1:]) # for j >= 1 \n",
    "    Theta2_grad[:,1:] = Theta2_grad[:, 1:] + ((reg/m) * Theta2[:,1:]) # for j >= 1 \n",
    "    \n",
    "    grad = np.concatenate((Theta1_grad.flatten(), Theta2_grad.flatten()))\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad = nnGradient(initial_nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 8.96387896e-03,  1.27428318e-05,  5.56429417e-07, -2.07790011e-05,\n",
       "        5.95264497e-06])"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Gradient checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeNumericalGradient(J, theta):\n",
    "    numgrad = np.zeros(theta.shape) \n",
    "    perturb = np.zeros(theta.shape) \n",
    "    e = 1e-4 \n",
    "    # Check the first 5 grads\n",
    "    for p in range(5):\n",
    "        # Set perturbation vector\n",
    "        perturb[p] = e\n",
    "        loss1 = J(theta - perturb, input_layer_size, hidden_layer_size, num_labels, X, y, reg) \n",
    "        loss2 = J(theta + perturb, input_layer_size, hidden_layer_size, num_labels, X, y, reg) \n",
    "        # Compute Numerical Gradient\n",
    "        numgrad[p] = (loss2 - loss1) / (2*e) \n",
    "        perturb[p] = 0\n",
    "    return numgrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_grad = computeNumericalGradient(nnCostFunction, initial_nn_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 8.96387895e-03,  1.27428290e-05,  5.56430457e-07, -2.07790052e-05,\n",
       "        5.95264282e-06])"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_grad[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Regularized neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 8.96387896e-03,  3.82284953e-05,  1.66928825e-06, -6.23325591e-05,\n",
       "        1.78636072e-05])"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([ 8.96387895e-03,  3.82284915e-05,  1.66929137e-06, -6.23325658e-05,\n",
       "        1.78636039e-05])"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg = 3\n",
    "initial_nn_params = np.concatenate((initial_Theta1.flatten(), initial_Theta2.flatten()))\n",
    "grad = nnGradient(initial_nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, reg)\n",
    "grad[:5]\n",
    "num_grad = computeNumericalGradient(nnCostFunction, initial_nn_params)\n",
    "num_grad[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = 3\n",
    "debug_J  = nnCostFunction(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5760512469501331"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "debug_J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
